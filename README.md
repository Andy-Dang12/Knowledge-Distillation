# Knowledge Distilling
Implementation of Distilling the Knowledge in a Neural Network https://arxiv.org/pdf/1503.02531.pdf

#### Reference
    * https://arxiv.org/pdf/1503.02531.pdf
    * https://github.com/wonbeomjang/Knowledge-Distilling-PyTorch/blob/master/loss.py
    * https://github.com/peterliht/knowledge-distillation-pytorch/blob/master/model/net.py
    * https://github.com/peterliht/knowledge-distillation-pytorch/issues/2
    * https://pytorch.org/docs/stable/generated/torch.nn.KLDivLoss.html
    * https://keras.io/examples/vision/knowledge_distillation/


#### TODO
- [ ] write custom dataset for outp_Teacher, see data.py
- [ ] write metric to caculator in training, accuracy, precision, recall, F1-score, see [link](https://machinelearningcoban.com/2017/08/31/evaluation/)
- [ ] write custom dataset for ImageFolder

